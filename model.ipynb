{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель с использованием библиотеки `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(3600, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape = (3,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(),loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотр построенной модели\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionXYZ import f_xy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_random_data(degrees, elems_in_degree):\n",
    "    \"\"\" Функция, генерирующая одинаковое число случайных элементов для каждой переданной степени.\n",
    "    Обоснование. Если брать просто рандомные числа в пределах от [-1000; 1000],\n",
    "    то чисел в пределах [-1;1] почти не будет, чисел в пределах [-10;10] будет очень мало\n",
    "    и так далее по аналогии, что приведёт к необучению младших разрядов.\n",
    "    Эту проблему и решает данная функция, возвращающая данные для тренировки модели с одинаковым числом нужных степеней.\n",
    "    \"\"\"\n",
    "    \n",
    "    for iters, degree in enumerate(degrees):\n",
    "        degree_multyplier = pow(10, degree)\n",
    "        X_data_tmp = np.random.rand(elems_in_degree, 2)*degree_multyplier*2-degree_multyplier\n",
    "        X_data_tmp = np.append(X_data_tmp, np.ones((elems_in_degree, 1)), axis=1)\n",
    "        X_data_tmp = X_data_tmp.reshape((elems_in_degree, 3))\n",
    "        X_data = np.append(X_data, X_data_tmp, 0) if iters > 0 else X_data_tmp\n",
    "    np.random.shuffle(X_data)\n",
    "    y_data = np.array([\n",
    "        f_xy(x[0], x[1]) for x in X_data\n",
    "    ]).reshape((elems_in_degree*len(degrees), 1))\n",
    "    return X_data, y_data\n",
    "\n",
    "# это номера разрядов 10, в рамках которых будет обучаться модель (не совсем так, но приблежено к истине)\n",
    "# не больше 4 разряда, на большее модель не хватает\n",
    "degrees = [-1, 0, 0.5]  # на самом деле, модель нормально обучается не больше чем только на трёх последовательных разрядах, например 1, 2, 3\n",
    "elems_in_degree = 5000  # число элементов на разряд\n",
    "X_k, y_k = prepare_random_data(degrees, elems_in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировка модели, 20% идут на проверку\n",
    "# Выполнить плитку раз 6-8, каждый раз с обновлёнными тренировочными данными (=выполнив плитку выше) (до loss равным 70-100)\n",
    "hist = model.fit(\n",
    "    X_k,\n",
    "    y_k,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение модели\n",
    "model.save('asl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так очищали память в курсе от nvidia, здесь это выдаёт странную ошибку\n",
    "import IPython\n",
    "\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# загрузка модели\n",
    "model = keras.models.load_model('asl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionXYZ import f_xy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def check_model(X):\n",
    "    model_result = model.predict(X)\n",
    "    function_result = [[f_xy(x[0], x[1])] for x in X]\n",
    "    for i in range(len(X)):\n",
    "        if function_result[i][0]:\n",
    "            error = np.absolute(\n",
    "                (np.absolute(model_result[i][0])-np.absolute(function_result[i][0]))\n",
    "                / (np.absolute(model_result[i][0])+np.absolute(function_result[i][0]))\n",
    "            )*100\n",
    "        else:\n",
    "            error = np.absolute(\n",
    "                (np.absolute(model_result[i][0])-np.absolute(function_result[i][0]))\n",
    "            )*100\n",
    "        print(\n",
    "            f'Got: {model_result[i][0]}, want: {function_result[i][0]},'\n",
    "            f' error: {error}%'\n",
    "        )\n",
    "\n",
    "\n",
    "# проверка нескольких примеров\n",
    "# каждый пример = [x, y, 1]\n",
    "check_X = [\n",
    "    [23, 54, 1],\n",
    "    [3, 7, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [0.32, 0.67, 1],\n",
    "    [1, -2, 1],\n",
    "    [1.01, -2.02, 1],\n",
    "]\n",
    "\n",
    "# этот пример также призван показать, что модель обучается только в конкретном диапозоне разрядов 10.\n",
    "check_model(check_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель буз использования библиотек (legacy code)\n",
    "\n",
    "Построенная модель без использования библиотек трудно настраивается на данные, не входящие в диапозон \\[-1;1\\].  \n",
    "Призвана показать, как строится простейшая модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, params_number, h_number):\n",
    "        self.weights1 = np.random.rand(params_number, h_number)\n",
    "        self.weights2 = np.random.rand(h_number, 1)\n",
    "\n",
    "    def feedforward(self, input_data):\n",
    "        self.layer1 = sigmoid(np.dot(input_data, self.weights1))\n",
    "        return sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self, input_data, output_data, check_data):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(\n",
    "            self.layer1.T,\n",
    "            (2*(check_data - output_data) * sigmoid_derivative(output_data))\n",
    "        )\n",
    "        d_weights1 = np.dot(\n",
    "            input_data.T,\n",
    "            (np.dot(\n",
    "                2*(check_data - output_data) * sigmoid_derivative(output_data),\n",
    "                self.weights2.T\n",
    "            ) * sigmoid_derivative(self.layer1))\n",
    "        )\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "\n",
    "    def train(self, epochs, input_data, check_data):\n",
    "        output_data = np.zeros(input_data.shape[0])\n",
    "        for _ in range(epochs):\n",
    "            output_data = self.feedforward(input_data)\n",
    "            self.backprop(input_data, output_data, check_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    return (x-1)**2+(y+2)**2\n",
    "\n",
    "h_number = 6\n",
    "params_number = 2\n",
    "nn = NeuralNetwork(params_number, h_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "values_number = 30\n",
    "max_x = 10\n",
    "# X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [-max_x, max_x]])\n",
    "X = np.random.rand(values_number, 2)*max_x\n",
    "y = np.array([\n",
    "    func(x[0], x[1]) for x in X\n",
    "]).reshape((values_number, 1))\n",
    "\n",
    "max_el = max(np.amax(np.absolute(X)), np.amax(np.absolute(y)))\n",
    "X = np.dot(X, 1/max_el)\n",
    "y = np.dot(y, 1/max_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(epochs, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nn.feedforward(X)\n",
    "z = np.dot(z, max_el)\n",
    "y = np.dot(y, max_el)\n",
    "for i in range(z.shape[0]):\n",
    "    error = np.absolute(\n",
    "        (np.absolute(z[i][0])-np.absolute(y[i][0]))\n",
    "        / (np.absolute(z[i][0])+np.absolute(y[i][0]))\n",
    "    )*100\n",
    "    print(\n",
    "        f'Got: {z[i][0]}, want: {y[i][0]},'\n",
    "        f' error: {error}%'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

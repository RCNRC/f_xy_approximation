{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель с использованием библиотеки `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()  # нейросеть последовательной архитектуры\n",
    "model.add(Dense(3600, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape = (3,)))  # регуляризатор применят штраф: loss = l2 * reduce_sum(square(x))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Оптимизация Adam — это метод стохастического градиентного спуска, основанный на адаптивной оценке моментов первого и второго порядка.\n",
    "# Согласно Kingma et al., 2014 , этот метод «эффективен в вычислительном отношении, требует мало памяти, инвариантен к диагональному масштабированию градиентов и хорошо подходит для задач, которые являются большими с точки зрения данных/параметров».\n",
    "model.compile(optimizer=Adam(),loss='mse')  # loss = square(y_true - y_pred) == Средняя квадратическая ошибка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 3600)              14400     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 3601      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,001\n",
      "Trainable params: 18,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# просмотр построенной модели\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionXYZ import f_xy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_random_data(degrees, elems_in_degree):\n",
    "    \"\"\" Функция, генерирующая одинаковое число случайных элементов для каждой переданной степени.\n",
    "    Обоснование. Если брать просто рандомные числа в пределах от [-1000; 1000],\n",
    "    то чисел в пределах [-1;1] почти не будет, чисел в пределах [-10;10] будет очень мало\n",
    "    и так далее по аналогии, что приведёт к необучению младших разрядов.\n",
    "    Эту проблему и решает данная функция, возвращающая данные для тренировки модели с одинаковым числом нужных степеней.\n",
    "    \"\"\"\n",
    "    \n",
    "    for iters, degree in enumerate(degrees):\n",
    "        degree_multyplier = pow(10, degree)\n",
    "        X_data_tmp = np.random.rand(elems_in_degree, 2)*degree_multyplier*2-degree_multyplier\n",
    "        X_data_tmp = np.append(X_data_tmp, np.ones((elems_in_degree, 1)), axis=1)\n",
    "        X_data_tmp = X_data_tmp.reshape((elems_in_degree, 3))\n",
    "        X_data = np.append(X_data, X_data_tmp, 0) if iters > 0 else X_data_tmp\n",
    "    np.random.shuffle(X_data)\n",
    "    y_data = np.array([\n",
    "        f_xy(x[0], x[1]) for x in X_data\n",
    "    ]).reshape((elems_in_degree*len(degrees), 1))\n",
    "    return X_data, y_data\n",
    "\n",
    "# это номера разрядов 10, в рамках которых будет обучаться модель (не совсем так, но приблежено к истине)\n",
    "# не больше 4 разряда, на большее модель не хватает\n",
    "degrees = [-1, 0, 0.5]  # на самом деле, модель нормально обучается не больше чем только на трёх последовательных разрядах, например 1, 2, 3\n",
    "elems_in_degree = 5000  # число элементов на разряд\n",
    "X_k, y_k = prepare_random_data(degrees, elems_in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 2/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 3/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 4/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 5/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 6/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 7/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 8/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 9/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 10/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 11/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 12/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 13/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 14/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 15/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 16/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 17/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 18/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 19/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 20/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 21/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 22/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 23/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 24/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 25/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 26/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 27/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 28/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 29/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 30/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 31/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 32/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 33/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 34/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 35/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 36/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 37/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 38/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 39/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 40/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 41/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 42/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 43/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 44/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 45/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 46/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 47/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 48/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 49/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 50/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 51/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 52/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 53/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 54/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 55/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 56/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 57/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 58/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 59/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 60/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 61/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 62/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 63/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 64/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 65/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 66/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 67/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 68/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 69/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 70/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 71/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 72/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 73/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 74/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 75/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 76/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 77/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 78/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 79/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 80/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 81/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 82/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 83/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 84/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 85/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 86/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 87/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 88/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 89/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 90/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 91/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 92/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 93/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 94/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 95/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 96/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 97/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 98/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 99/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.0033\n",
      "Epoch 100/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 101/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 102/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 103/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 104/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 105/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 106/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 107/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 108/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 109/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 110/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 111/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 112/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 113/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 114/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 115/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0040\n",
      "Epoch 116/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 117/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 118/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 119/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 120/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 121/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 122/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 123/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 124/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 125/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 126/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 127/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 128/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 129/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 130/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 131/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 132/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 133/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 134/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 135/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 136/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 137/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 138/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 139/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 140/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 141/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 142/150\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 143/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 144/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 145/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 146/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 147/150\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 148/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 149/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 150/150\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# Тренировка модели, 20% идут на проверку\n",
    "# Выполнить плитку 6-8 раз\n",
    "hist = model.fit(\n",
    "    X_k,\n",
    "    y_k,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение модели\n",
    "model.save('asl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так очищали память в курсе от nvidia, здесь это выдаёт странную ошибку\n",
    "import IPython\n",
    "\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# загрузка модели\n",
    "model = keras.models.load_model('asl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Got: 608.4143676757812, want: 3620, error: 494.9892363372107%\n",
      "Got: 68.45489501953125, want: 85, error: 24.16935264563356%\n",
      "Got: 9.997167587280273, want: 10, error: 0.028332152032040092%\n",
      "Got: 3.9990663528442383, want: 4, error: 0.023346628272302716%\n",
      "Got: 9.001907348632812, want: 9, error: 0.021192762586808023%\n",
      "Got: 7.5903496742248535, want: 7.5912999999999995, error: 0.012520184391151545%\n",
      "Got: 0.0015122704207897186, want: 0, error: 0.15122704207897186%\n",
      "Got: 0.0029009953141212463, want: 0.0005000000000000009, error: 480.1990628242483%\n"
     ]
    }
   ],
   "source": [
    "from functionXYZ import f_xy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def check_model(X):\n",
    "    model_result = model.predict(X)\n",
    "    function_result = [[f_xy(x[0], x[1])] for x in X]\n",
    "    for i in range(len(X)):\n",
    "        if function_result[i][0]:\n",
    "            max_res = max(model_result[i][0], function_result[i][0])\n",
    "            min_res = min(model_result[i][0], function_result[i][0])\n",
    "            error = (max_res/min_res - 1)*100\n",
    "        else:  # function_result[i][0] == 0\n",
    "            error = np.absolute(model_result[i][0])*100\n",
    "        print(\n",
    "            f'Got: {model_result[i][0]}, want: {function_result[i][0]},'\n",
    "            f' error: {error}%'\n",
    "        )\n",
    "\n",
    "\n",
    "# проверка нескольких примеров\n",
    "# каждый пример = [x, y, 1]\n",
    "check_X = [\n",
    "    [23, 54, 1],\n",
    "    [3, 7, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [0.32, 0.67, 1],\n",
    "    [1, -2, 1],\n",
    "    [1.01, -2.02, 1],\n",
    "]\n",
    "\n",
    "# этот пример также призван показать, что модель обучается только в конкретном диапозоне разрядов 10.\n",
    "check_model(check_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель буз использования библиотек (legacy code)\n",
    "\n",
    "Построенная модель без использования библиотек трудно настраивается на данные, не входящие в диапозон \\[-1;1\\].  \n",
    "Призвана показать, как строится простейшая модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, params_number, h_number):\n",
    "        self.weights1 = np.random.rand(params_number, h_number)\n",
    "        self.weights2 = np.random.rand(h_number, 1)\n",
    "\n",
    "    def feedforward(self, input_data):\n",
    "        self.layer1 = sigmoid(np.dot(input_data, self.weights1))\n",
    "        return sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self, input_data, output_data, check_data):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(\n",
    "            self.layer1.T,\n",
    "            (2*(check_data - output_data) * sigmoid_derivative(output_data))\n",
    "        )\n",
    "        d_weights1 = np.dot(\n",
    "            input_data.T,\n",
    "            (np.dot(\n",
    "                2*(check_data - output_data) * sigmoid_derivative(output_data),\n",
    "                self.weights2.T\n",
    "            ) * sigmoid_derivative(self.layer1))\n",
    "        )\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "\n",
    "    def train(self, epochs, input_data, check_data):\n",
    "        output_data = np.zeros(input_data.shape[0])\n",
    "        for _ in range(epochs):\n",
    "            output_data = self.feedforward(input_data)\n",
    "            self.backprop(input_data, output_data, check_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    return (x-1)**2+(y+2)**2\n",
    "\n",
    "h_number = 6\n",
    "params_number = 2\n",
    "nn = NeuralNetwork(params_number, h_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "values_number = 30\n",
    "max_x = 10\n",
    "# X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [-max_x, max_x]])\n",
    "X = np.random.rand(values_number, 2)*max_x\n",
    "y = np.array([\n",
    "    func(x[0], x[1]) for x in X\n",
    "]).reshape((values_number, 1))\n",
    "\n",
    "max_el = max(np.amax(np.absolute(X)), np.amax(np.absolute(y)))\n",
    "X = np.dot(X, 1/max_el)\n",
    "y = np.dot(y, 1/max_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(epochs, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: 55.86547245682782, want: 85.32176994698325, error: 20.86328551265785%\n",
      "Got: 61.49856361753632, want: 64.03659068299694, error: 2.021765998219547%\n",
      "Got: 7.170559847577435, want: 9.17737650827642, error: 12.275657410302964%\n",
      "Got: 139.6569293971853, want: 140.72095592618874, error: 0.3794973087040129%\n",
      "Got: 150.23073050446288, want: 164.98484836654234, error: 4.680643613784469%\n",
      "Got: 143.79623999717776, want: 147.68750975075648, error: 1.334986858424786%\n",
      "Got: 16.105155264218016, want: 18.156545505300286, error: 5.987415087424195%\n",
      "Got: 146.4976684778596, want: 158.56977527088105, error: 3.9571927586492204%\n",
      "Got: 160.1252015512418, want: 187.3637241262987, error: 7.838673569797004%\n",
      "Got: 132.4643700132971, want: 135.6398672783728, error: 1.184426362355881%\n",
      "Got: 141.0518672522793, want: 142.52215822368098, error: 0.518485770667428%\n",
      "Got: 23.776866853634758, want: 28.735287531989567, error: 9.442424780256628%\n",
      "Got: 62.721584786086446, want: 80.42077550861036, error: 12.3647470155483%\n",
      "Got: 132.84615019772366, want: 135.4427493287829, error: 0.9678369606949375%\n",
      "Got: 92.10014567944971, want: 85.6450163967655, error: 3.631676500942581%\n",
      "Got: 65.471296488708, want: 67.56501653051723, error: 1.573795901504467%\n",
      "Got: 125.1050377895181, want: 119.56804948702002, error: 2.263014851420898%\n",
      "Got: 71.00466857636589, want: 68.2584369478, error: 1.9719735663149742%\n",
      "Got: 106.4601507490271, want: 97.39419402903684, error: 4.447271766446947%\n",
      "Got: 60.19138877993901, want: 76.07311853781819, error: 11.655074436107078%\n",
      "Got: 119.99890981562012, want: 139.71255022364917, error: 7.59059319332627%\n",
      "Got: 141.96146134972585, want: 147.66447099610727, error: 1.9690949633514292%\n",
      "Got: 10.559942280377673, want: 11.823065180811849, error: 5.643222442847941%\n",
      "Got: 123.74065324435077, want: 128.50291870958398, error: 1.8879630621877288%\n",
      "Got: 110.51470591358573, want: 101.8624383066438, error: 4.074010712739295%\n",
      "Got: 105.09834375705562, want: 113.02184992579778, error: 3.6326330152919897%\n",
      "Got: 37.52605140898674, want: 47.98093261289086, error: 12.226932482180827%\n",
      "Got: 18.95331496552961, want: 27.93094272838186, error: 19.14849078226552%\n",
      "Got: 45.81485849495825, want: 45.95764648093174, error: 0.15558906887308355%\n",
      "Got: 33.01923379828647, want: 35.065145172578035, error: 3.0049644356263787%\n"
     ]
    }
   ],
   "source": [
    "z = nn.feedforward(X)\n",
    "z = np.dot(z, max_el)\n",
    "y = np.dot(y, max_el)\n",
    "for i in range(z.shape[0]):\n",
    "    error = np.absolute(\n",
    "        (np.absolute(z[i][0])-np.absolute(y[i][0]))\n",
    "        / (np.absolute(z[i][0])+np.absolute(y[i][0]))\n",
    "    )*100\n",
    "    print(\n",
    "        f'Got: {z[i][0]}, want: {y[i][0]},'\n",
    "        f' error: {error}%'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
